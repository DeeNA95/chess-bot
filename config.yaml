# Algorithm Selection: 'mcts', 'grpo', 'ppo', 'ppo_mcts', "grpo_mcts"
algorithm: "ppo_mcts"

# Model Architecture
model:
  embed_dim: 384
  num_layers: 8
  num_heads: 8
  mlp_ratio: 4.0
  dropout: 0.1

# Reward Settings
rewards:
  stockfish_path: "/usr/games/stockfish"
  stockfish_depth: 3
  stockfish_hash: 128
  num_workers: 32
  stockfish_weight: 0.7
  material_weight: 0.2
  outcome_weight: 0.1

# General Training Settings
training:
  total_games: 100000
  batch_size: 1024
  games_per_update: 2048
  lr: 0.0001
  checkpoint_dir: "./checkpoints"
  buffer_capacity: 500000
  num_parallel_games: 1024 # 3090 can handle large batches, 96 cores can drive MCTS
  device: "cuda"

# Self-play worker settings (multi-process)
# Set num_workers to 0 to use optimized Single-Process Batched Self-Play (Recommended for GPU)
self_play:
  num_workers: 0
  games_per_worker: 4 # Ignored if num_workers=0, num_parallel_games used instead
  max_moves: 200
  flush_every_moves: 10
  sync_weights_every: 1

# MCTS Specific Settings
mcts:
  num_simulations: 400
  c_puct: 1.5
  temperature: 1.0
  dirichlet_alpha: 0.03
  dirichlet_epsilon: 0.25
  reuse_tree: true
  leaves_per_sim: 8
  max_nodes_per_tree: 300000

# GRPO Specific Settings
grpo:
  group_size: 16      # G in GRPO paper (outcomes sampled per state)
  epsilon: 0.2        # Clipping parameter
  beta_kl: 0.01       # KL penalty coefficient
  entropy_coef: 0.01  # Entropy bonus

# PPO Specific Settings
ppo:
  clip_ratio: 0.2
  value_coef: 0.5
  entropy_coef: 0.01
  gae_lambda: 0.95
  gamma: 0.99
  ppo_epochs: 4
